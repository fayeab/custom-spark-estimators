{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Pyspark : Creation d'estimators et transformers </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "conf = SparkConf().setAppName('Exple')\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un pipeline est une séquence d'étapes finies, chaque étape est soit un transformer ou estimator.\n",
    "* Un transformer est un processus qui permet de transformer un dataframe à un autre nécessitant l'implémentation d'un `transformer()`:\n",
    "    * Exemples\n",
    "        * Tokenization: segmenter un texte en une liste de mots.\n",
    "        * On peut créer un transformer qui supprime les variables corrlélées d'un dataframe\n",
    "* Un estimator est un opérateur qui prend en entrée un dataframe et retourne un transformer. Il necessite l'implémentation d'un `fit()` et d'un `transformer()`\n",
    "    * Exemples :\n",
    "        * VectorIndexer: coder des catégories\n",
    "        * Un algorithme ML de SparkML\n",
    "\n",
    "Après le fitting, un pipeline retourne un pipelineModel (transformer). Chaque estimator du pipeline intial devient un transformer dans le pipelineModel. \n",
    "\n",
    "Spark dispose d'un API spécifique permettant le partage de parametres entre estimators et transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| ID|CATEGORY|\n",
      "+---+--------+\n",
      "|  0|      aa|\n",
      "|  1|      ab|\n",
      "|  2|      cd|\n",
      "|  3|      ad|\n",
      "|  4|      aa|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# créer une dataFrame à partir d’une séquence\n",
    "\n",
    "seq =  [(0, \"aa\"), (1, \"ab\"), (2, \"cd\"), (3, \"ad\"), (4, \"aa\")]\n",
    "dfm = spark.createDataFrame(seq).toDF(\"ID\", \"CATEGORY\")\n",
    "dfm.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pyspark Estimators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.sql.functions import col, create_map, lit\n",
    "from pyspark.ml.param import Params\n",
    "from pyspark.ml.param.shared import TypeConverters, Param\n",
    "from pyspark.ml.pipeline import Estimator, Model, Pipeline, DefaultParamsReadable, DefaultParamsWritable\n",
    "from itertools import chain\n",
    "\n",
    "class HasInOutputCol(Params):\n",
    "    \"\"\"\n",
    "    Mixin for param columns\n",
    "    \"\"\"\n",
    "\n",
    "    inputCol = Param(Params._dummy(), \"inputCol\", \n",
    "                      \"Input column\", \n",
    "                      typeConverter=TypeConverters.toString)\n",
    "    outputCol = Param(Params._dummy(), \"outputCol\", \n",
    "                      \"Output column name\", \n",
    "                      typeConverter=TypeConverters.toString)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasInOutputCol, self).__init__()\n",
    "        self._setDefault(inputCol=None, outputCol=None)\n",
    "\n",
    "    def getInputCol(self):\n",
    "        \"\"\"\n",
    "        Gets the list of input columns.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.inputCol)\n",
    "\n",
    "    def getOutputCol(self):\n",
    "        \"\"\"\n",
    "        Gets the list of output columns.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.outputCol)\n",
    "    \n",
    "class HasMappingCategories(Params):\n",
    "    \"\"\"\n",
    "    Mixin for param mappingCat\n",
    "    \"\"\"\n",
    "\n",
    "    mappingCat = Param(Params._dummy(), \"mappingCat\", \n",
    "                        \"Mapping category <=> Double\", \n",
    "                        typeConverter=TypeConverters.toList)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasMappingCategories, self).__init__()\n",
    "        self._setDefault(mappingCat=None)\n",
    "\n",
    "    def getMappingCat(self):\n",
    "        \"\"\"\n",
    "        Gets the list of columns to drop.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.mappingCat)\n",
    "\n",
    "    def setMappingCat(self, value):\n",
    "        \"\"\"\n",
    "        Gets the list of columns to drop.\n",
    "        \"\"\"\n",
    "        return self._set(mappingCat=value)\n",
    "\n",
    "class CustomVectorIndexer(Estimator, HasInOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(CustomVectorIndexer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)  \n",
    "        self._setDefault(inputCol=None, outputCol=None)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "    \n",
    "    def setInputCol(self, value):\n",
    "        \"\"\"\n",
    "        Sets the value of :py:attr:`inputCol`.\n",
    "        \"\"\"\n",
    "        return self._set(inputCol=value)\n",
    "    \n",
    "    def setOutputCol(self, value):\n",
    "        \"\"\"\n",
    "        Sets the value of :py:attr:`outputCol`.\n",
    "        \"\"\"\n",
    "        return self._set(outputCol=value)\n",
    "\n",
    "    def _fit(self, dataset):\n",
    "        column = self.getInputCol()\n",
    "        mapping_cat = sorted([cat[column] \n",
    "            for cat in dataset.select(column).distinct().collect()])\n",
    "        mapping_cat = [(value, key) for key, value in enumerate(mapping_cat)]\n",
    "        return CustomVectorIndexerModel(inputCol=self.getInputCol(),\n",
    "                                        outputCol=self.getOutputCol(),\n",
    "                                        mappingCat=mapping_cat)  \n",
    "\n",
    "\n",
    "class CustomVectorIndexerModel(Model, HasMappingCategories, HasInOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None, mappingCat=None):\n",
    "        super(CustomVectorIndexerModel, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None, mappingCat=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        input_col = self.getInputCol()\n",
    "        output_col = self.getOutputCol()\n",
    "        mapping = create_map([lit(col) for col in chain(*self.getMappingCat())])\n",
    "        return dataset.withColumn(output_col, mapping[col(input_col)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+---------------+\n",
      "|ID |CATEGORY|CATEGORY_FITTED|\n",
      "+---+--------+---------------+\n",
      "|0  |aa      |0              |\n",
      "|1  |ab      |1              |\n",
      "|2  |cd      |3              |\n",
      "|3  |ad      |2              |\n",
      "|4  |aa      |0              |\n",
      "+---+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CustomVectorIndexer().setInputCol('CATEGORY').setOutputCol('CATEGORY_FITTED')\n",
    "model  = vectorizer.fit(dfm)\n",
    "model.transform(dfm).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pyspark - Transformers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.ml import Pipeline, Transformer, Estimator, Model\n",
    "\n",
    "class HasDropCol(Params):\n",
    "    \"\"\"\n",
    "    Mixin for param column\n",
    "    \"\"\"\n",
    "\n",
    "    column = Param(Params._dummy(), \"column\", \n",
    "                      \"Column to drop\", \n",
    "                      typeConverter=TypeConverters.toString)\n",
    "\n",
    "    def __init__(self):\n",
    "        super(HasDropCol, self).__init__()\n",
    "        self._setDefault(column=None)\n",
    "\n",
    "    def getDropCol(self):\n",
    "        \"\"\"\n",
    "        Gets the column to drop.\n",
    "        \"\"\"\n",
    "        return self.getOrDefault(self.column)\n",
    "\n",
    "class ColumnFilter(Transformer, HasDropCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    \"\"\"\n",
    "    Filter columns.\n",
    "    \"\"\"\n",
    "    def __init__(self, column=None):\n",
    "        \"\"\"\n",
    "        __init__(self, column=None)\n",
    "        \"\"\"\n",
    "        super(ColumnFilter, self).__init__()\n",
    "        self._setDefault(column=column)\n",
    "\n",
    "    \n",
    "    def setDropCol(self, value):\n",
    "        \"\"\"\n",
    "        Sets the value of :py:attr:`columns`.\n",
    "        \"\"\"\n",
    "        return self._set(column=value)\n",
    "    \n",
    "    def _transform(self, dataset):\n",
    "        \"\"\"\n",
    "        Drop columns.\n",
    "        \"\"\"\n",
    "        column = self.getDropCol()\n",
    "        if column:\n",
    "            return dataset.drop(column)\n",
    "        else:\n",
    "            return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|CATEGORY|\n",
      "+--------+\n",
      "|      aa|\n",
      "|      ab|\n",
      "|      cd|\n",
      "|      ad|\n",
      "|      aa|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filter_ = ColumnFilter().setDropCol('ID')\n",
    "filter_.transform(dfm).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------------+\n",
      "|ID |CATEGORY_FITTED|\n",
      "+---+---------------+\n",
      "|0  |0              |\n",
      "|1  |1              |\n",
      "|2  |3              |\n",
      "|3  |2              |\n",
      "|4  |0              |\n",
      "+---+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CustomVectorIndexer().setInputCol('CATEGORY').setOutputCol('CATEGORY_FITTED')\n",
    "filter_ = ColumnFilter().setDropCol('CATEGORY')\n",
    "model  = Pipeline(stages=[vectorizer, filter_]).fit(dfm)\n",
    "model.transform(dfm).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='red'>Remarque : </font>\n",
    "\n",
    "1. Les estimators et transformers fonctionnent correctement. Cependant les modèles créés ne peuvent pas être sauvagardés et utilsés ultérieurment.\n",
    "\n",
    "\n",
    "2. Une solution serait de s'inspirer du développement de Pyspark :    \n",
    "    a. Les estimators et transformers sont créés en Scala    \n",
    "    b. Pour utiliser ces objets avec Python, on crée un wrapper"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
