{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'> Spark: Création d'estimators et transformers </font>\n",
    "\n",
    "L'objectif est de montrer avec un exemple simple commment créer ses propres estimators et transformers avec Scala.\n",
    "\n",
    "1. L'estimator et le transformer sont d'abord créés en Scala \n",
    "2. Ensuite les classes sont packagées en créant un JAR contenant les classes et leurs dépendances (via plugin assembly de SBT)\n",
    "2. Le fichier JAR doit être accessible pour Spark (via la classpath de Spark par exemple)\n",
    "\n",
    "Le code est [ici](src\\main\\scala)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+\n",
      "|employee_name|department|salary|\n",
      "+-------------+----------+------+\n",
      "|        James|     Sales|  3000|\n",
      "|      Michael|     Sales|  4600|\n",
      "|       Robert|     Sales|  4100|\n",
      "|        Maria|   Finance|  3000|\n",
      "|        James|     Sales|  3000|\n",
      "|        Scott|   Finance|  3300|\n",
      "|          Jen|   Finance|  3900|\n",
      "|         Jeff| Marketing|  3000|\n",
      "|        Kumar| Marketing|  2000|\n",
      "|         Saif|     Sales|  4100|\n",
      "+-------------+----------+------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import spark.implicits._\r\n",
       "simpleData: Seq[(String, String, Int)] = List((James,Sales,3000), (Michael,Sales,4600), (Robert,Sales,4100), (Maria,Finance,3000), (James,Sales,3000), (Scott,Finance,3300), (Jen,Finance,3900), (Jeff,Marketing,3000), (Kumar,Marketing,2000), (Saif,Sales,4100))\r\n",
       "df: org.apache.spark.sql.DataFrame = [employee_name: string, department: string ... 1 more field]\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "val simpleData = Seq((\"James\", \"Sales\", 3000),\n",
    "    (\"Michael\", \"Sales\", 4600),\n",
    "    (\"Robert\", \"Sales\", 4100),\n",
    "    (\"Maria\", \"Finance\", 3000),\n",
    "    (\"James\", \"Sales\", 3000),\n",
    "    (\"Scott\", \"Finance\", 3300),\n",
    "    (\"Jen\", \"Finance\", 3900),\n",
    "    (\"Jeff\", \"Marketing\", 3000),\n",
    "    (\"Kumar\", \"Marketing\", 2000),\n",
    "    (\"Saif\", \"Sales\", 4100)\n",
    "  )\n",
    "\n",
    "val df = simpleData.toDF(\"employee_name\", \"department\", \"salary\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Estimator pour remplacer les modalités d'une variable catégorielle par des entiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+---------------+\n",
      "|employee_name|department|salary|departmentIndex|\n",
      "+-------------+----------+------+---------------+\n",
      "|        James|     Sales|  3000|            2.0|\n",
      "|      Michael|     Sales|  4600|            2.0|\n",
      "|       Robert|     Sales|  4100|            2.0|\n",
      "|        Maria|   Finance|  3000|            0.0|\n",
      "|        James|     Sales|  3000|            2.0|\n",
      "|        Scott|   Finance|  3300|            0.0|\n",
      "|          Jen|   Finance|  3900|            0.0|\n",
      "|         Jeff| Marketing|  3000|            1.0|\n",
      "|        Kumar| Marketing|  2000|            1.0|\n",
      "|         Saif|     Sales|  4100|            2.0|\n",
      "+-------------+----------+------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import feature.CustomStringIndexer\r\n",
       "indexer: feature.CustomStringIndexer = cstri_383fc9739cb5\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feature.CustomStringIndexer\n",
    "val indexer = new CustomStringIndexer()\n",
    "                  .setInputCol(\"department\")\n",
    "                  .setOutputCol(\"departmentIndex\")\n",
    "\n",
    "indexer.fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: feature.CustomStringIndexerModel = CustomStringIndexerModel: uid=cstri_383fc9739cb5\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model = indexer.fit(df)\n",
    "model.write.overwrite().save(\"spark-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+------+---------------+\n",
      "|employee_name|department|salary|departmentIndex|\n",
      "+-------------+----------+------+---------------+\n",
      "|        James|     Sales|  3000|            2.0|\n",
      "|      Michael|     Sales|  4600|            2.0|\n",
      "|       Robert|     Sales|  4100|            2.0|\n",
      "|        Maria|   Finance|  3000|            0.0|\n",
      "|        James|     Sales|  3000|            2.0|\n",
      "|        Scott|   Finance|  3300|            0.0|\n",
      "|          Jen|   Finance|  3900|            0.0|\n",
      "|         Jeff| Marketing|  3000|            1.0|\n",
      "|        Kumar| Marketing|  2000|            1.0|\n",
      "|         Saif|     Sales|  4100|            2.0|\n",
      "+-------------+----------+------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import feature.CustomStringIndexerModel\r\n",
       "modelLoaded: feature.CustomStringIndexerModel = CustomStringIndexerModel: uid=cstri_383fc9739cb5\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feature.CustomStringIndexerModel\n",
    "val modelLoaded = CustomStringIndexerModel.load(\"spark-model\")\n",
    "modelLoaded.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformer pour supprimer une colonne d'un dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|employee_name|department|\n",
      "+-------------+----------+\n",
      "|        James|     Sales|\n",
      "|      Michael|     Sales|\n",
      "|       Robert|     Sales|\n",
      "|        Maria|   Finance|\n",
      "|        James|     Sales|\n",
      "|        Scott|   Finance|\n",
      "|          Jen|   Finance|\n",
      "|         Jeff| Marketing|\n",
      "|        Kumar| Marketing|\n",
      "|         Saif|     Sales|\n",
      "+-------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import feature.ColumnFilter\r\n",
       "filter: feature.ColumnFilter =\r\n",
       "ColumnFilter: uid=colfilter_0776de12ebcd\r\n",
       "Drop: salary\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feature.ColumnFilter\n",
    "val filter = new ColumnFilter().setDropCols(\"salary\")\n",
    "filter.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter.write.overwrite().save(\"spark-filter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+\n",
      "|employee_name|department|\n",
      "+-------------+----------+\n",
      "|        James|     Sales|\n",
      "|      Michael|     Sales|\n",
      "|       Robert|     Sales|\n",
      "|        Maria|   Finance|\n",
      "|        James|     Sales|\n",
      "|        Scott|   Finance|\n",
      "|          Jen|   Finance|\n",
      "|         Jeff| Marketing|\n",
      "|        Kumar| Marketing|\n",
      "|         Saif|     Sales|\n",
      "+-------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "transLoad: feature.ColumnFilter =\r\n",
       "ColumnFilter: uid=colfilter_0776de12ebcd\r\n",
       "Drop: salary\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val transLoad = ColumnFilter.load(\"spark-filter\")\n",
    "transLoad.transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Les ajouter dans un pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------+---------------+\n",
      "|employee_name|salary|departmentIndex|\n",
      "+-------------+------+---------------+\n",
      "|        James|  3000|            2.0|\n",
      "|      Michael|  4600|            2.0|\n",
      "|       Robert|  4100|            2.0|\n",
      "|        Maria|  3000|            0.0|\n",
      "|        James|  3000|            2.0|\n",
      "|        Scott|  3300|            0.0|\n",
      "|          Jen|  3900|            0.0|\n",
      "|         Jeff|  3000|            1.0|\n",
      "|        Kumar|  2000|            1.0|\n",
      "|         Saif|  4100|            2.0|\n",
      "+-------------+------+---------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\r\n",
       "indexer: feature.CustomStringIndexer = cstri_9bb58bd35519\r\n",
       "filter: feature.ColumnFilter =\r\n",
       "ColumnFilter: uid=colfilter_82bde31a7e32\r\n",
       "Drop: department\r\n",
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_fdc2fa30ea6f\r\n",
       "model: org.apache.spark.ml.PipelineModel = pipeline_fdc2fa30ea6f\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val indexer = new CustomStringIndexer()\n",
    "                  .setInputCol(\"department\")\n",
    "                  .setOutputCol(\"departmentIndex\")\n",
    "\n",
    "val filter = new ColumnFilter().setDropCols(\"department\")\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "              .setStages(Array(indexer, filter))\n",
    "\n",
    "val model = pipeline.fit(df)\n",
    "\n",
    "model.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write.overwrite().save(\"spark-pipeline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sources :**     \n",
    "[Custom Spark ML with a Python wrapper](https://raufer.github.io/2018/02/08/custom-spark-models-with-python-wrappers/)    \n",
    "[Custom transformer for Apache Spark and MLeap](https://medium.com/@m.majidpour/custom-transformer-for-apache-spark-and-mleap-66b95ad0d37b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
